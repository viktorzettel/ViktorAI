import numpy as np
import random
import matplotlib.pyplot as plt

class GridWorld:
    def __init__(self, size=5, start=(0, 0), goal=(4, 4)):
        self.size = size
        self.start = start
        self.goal = goal
        self.state = start  # Current position
        self.actions = ['up', 'down', 'left', 'right']  # Possible moves

    def reset(self):
        self.state = self.start
        return self.state

    def step(self, action):
        row, col = self.state
        if action == 'up':
            row = max(0, row - 1)
        elif action == 'down':
            row = min(self.size - 1, row + 1)
        elif action == 'left':
            col = max(0, col - 1)
        elif action == 'right':
            col = min(self.size - 1, col + 1)

        self.state = (row, col)
        reward = -1  # Default step cost
        done = False
        if self.state == self.goal:
            reward = 10
            done = True

        return self.state, reward, done

    def is_terminal(self, state):
        return state == self.goal

    def render_path(self, path):
        """Print a text-based grid showing the path with step numbers."""
        grid = [['.' for _ in range(self.size)] for _ in range(self.size)]
        grid[self.start[0]][self.start[1]] = 'S'
        grid[self.goal[0]][self.goal[1]] = 'G'

        for i, (r, c) in enumerate(path[1:]):  # Skip start
            grid[r][c] = str((i + 1) % 10)  # Number steps, modulo 10 for single digit

        for row in grid:
            print(' '.join(row))
        print()

def choose_action(state, q_table, epsilon, actions):
    """Epsilon-greedy action selection."""
    if random.uniform(0, 1) < epsilon:
        return random.choice(actions)  # Explore
    else:
        action_idx = np.argmax(q_table[state[0], state[1]])  # Exploit
        return actions[action_idx]

def train_q_learning(env, num_episodes, epsilon, alpha=0.1, gamma=0.9, decay_epsilon=False):
    """Train the agent using Q-learning."""
    actions = env.actions
    q_table = np.zeros((env.size, env.size, len(actions)))  # Q-table: rows x cols x actions
    episode_steps = []  # Track steps per episode for learning curve
    sample_paths = {}  # Store paths for high and low epsilon demos

    current_epsilon = epsilon
    min_epsilon = 0.01
    decay_rate = 0.995 if decay_epsilon else 1.0  # No decay if fixed

    for episode in range(num_episodes):
        state = env.reset()
        path = [state]  # Record path for visualization
        steps = 0
        done = False

        while not done and steps < 100:  # Max steps to prevent loops
            action = choose_action(state, q_table, current_epsilon, actions)
            next_state, reward, done = env.step(action)
            path.append(next_state)

            # Q-update
            action_idx = actions.index(action)
            next_max = np.max(q_table[next_state[0], next_state[1]])
            q_table[state[0], state[1], action_idx] += alpha * (reward + gamma * next_max - q_table[state[0], state[1], action_idx])

            state = next_state
            steps += 1

        episode_steps.append(steps)

        # Decay epsilon if enabled
        current_epsilon = max(min_epsilon, current_epsilon * decay_rate)

        # Save sample paths: one early (high epsilon), one late (low epsilon)
        if episode == 0:
            sample_paths['high_epsilon'] = (path, epsilon)
        if episode == num_episodes - 1:
            sample_paths['low_epsilon'] = (path, current_epsilon)

    return q_table, episode_steps, sample_paths

def plot_learning_curve(episode_steps):
    """Plot steps per episode to show learning progress."""
    plt.plot(episode_steps)
    plt.title('Learning Curve: Steps per Episode')
    plt.xlabel('Episode')
    plt.ylabel('Steps to Goal')
    plt.show()

def print_full_q_table(q_table, actions, size):
    """Print the full Q(s, a) table for all states and actions."""
    print("\nFull Q-Table (Q(s, a)):")
    for row in range(size):
        for col in range(size):
            state = (row, col)
            q_values = q_table[row, col]
            print(f"State {state}:")
            for i, action in enumerate(actions):
                print(f"  {action}: {q_values[i]:.2f}")
            print()

def main():
    # User inputs
    print("Welcome to GridWorld RL Demo!")
    epsilon = float(input("Enter exploration weight (epsilon, e.g., 0.7 for 70% exploration): "))
    exploitation = 1 - epsilon  # Just for display
    print(f"Exploitation weight: {exploitation:.2f}")
    num_episodes = int(input("Enter number of episodes to train (e.g., 100): "))
    use_decay = input("Use epsilon decay over episodes? (y/n): ").lower() == 'y'

    # Create environment
    env = GridWorld(size=5)

    # Train
    q_table, episode_steps, sample_paths = train_q_learning(env, num_episodes, epsilon, decay_epsilon=use_decay)

    # Visualize sample paths
    print("\nSample Path with High Epsilon (Exploration-heavy):")
    path, eps = sample_paths['high_epsilon']
    print(f"Epsilon: {eps:.2f}, Steps: {len(path) - 1}")
    env.render_path(path)

    print("Sample Path with Low Epsilon (Exploitation-heavy):")
    path, eps = sample_paths['low_epsilon']
    print(f"Epsilon: {eps:.2f}, Steps: {len(path) - 1}")
    env.render_path(path)

    # Plot learning curve
    plot_learning_curve(episode_steps)

    # Print final max Q-values (as before)
    print("\nFinal Max Q-Values per State (Higher near goal):")
    for row in range(env.size):
        print(' '.join(f"{np.max(q_table[row, col]):.2f}" for col in range(env.size)))

    # New: Print the full Q-table
    print_full_q_table(q_table, env.actions, env.size)

if __name__ == "__main__":
    main()
